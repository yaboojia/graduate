\cleardoublepage
\chapternonum{摘要}
随着智能设备和互联网的发展，终端设备产生了越来越多的数据，其中包含着各种各样的用户隐私。
而如今模型的训练需要越来越多的数据，这些数据依赖于终端设备用户产生。
随着公众对于数据安全的担忧加剧，将终端设备数据集中到服务器端变得异常困难。
联邦学习（Federated Learning, FL）提出的非中心化训练框架有效地解决了此问题，
在联邦学习中所有的终端设备合作训练模型，具体来说，
终端设备使用本地数据训练模型，
然后将训练完成的模型上传到服务器端聚合。
然而，随着模型规模的不断增大，
多数的终端设备受限于计算机资源与计算机性能不能完整的训练整个模型。
这将导致很多数据无法参与模型训练。

为了解决资源受限引起的边缘设备无法训练模型的问题，
提出了在边缘设备上训练全局模型的子模型的方法。
本文基于子模型抽取方向提出了两种不同方法，
分别解决了客户端分布质量不同场景下的资源受限联邦学习问题。
首先，提出了基于数据分布的子模型抽取
联邦学习范式（Distribution-aware Sub-model Extraction, FedDSE）。
相较于根据预定策略抽取子模型引起的神经元分别朝不同方向优化所造成的竞争关系，
根据客户终端设备数据的不同在全局模型上激活的神经元的不同来挑选激活值高的神经元，
组成子模型，
在每个训练轮次都能自动根据边缘设备数据分布不同抽取特定的子模型，
有效解决了子模型训练造成的神经元冲突。
经过大量实验证明，
相较于其他方法FedDSE在低质量客户端分布场景下模型效果取得显著提升。
其次，在高客户端分布场景中通过约束全局模型与子模型梯度更新方向，
提出了一种基于梯度的子模型抽取的联邦学习
（Grad-based Sub-model Extraction, FedGSE）。
此方法考虑在每个轮次的更新中，
挑选出全局模型在反向传播过程中梯度大的神经元来组成子模型，
确保每次子模型的更新与全局模型的更新最接近，
从而保证了子模型与全局模型朝着相同方向优化。
在多个数据集上FedGSE对比以往的方法均有较大的提升。

\textbf{关键词：} 联邦学习；子模型抽取；分布感知；梯度更新优化




\cleardoublepage
\chapternonum{Abstract}
With the development of smart devices and the internet, 
terminal devices generate increasing amounts of data, which contain various types of user privacy.
At the same time, model training requires more and more data, 
which depends on the data generated by users of terminal devices. 
As public concerns about data security intensify, 
centralizing terminal device data on servers has become exceptionally difficult.
Federated Learning (FL), a decentralized training framework, effectively addresses this issue.
In FL,
all terminal devices collaborate to train the model, where each device uses local data to train the model, 
and the model is aggregated on the server. 
However, as the model parameters continue to grow, many terminal devices, 
limited by their computing resources and performance, 
are unable to fully train the entire model. 
This results in much data being excluded from the model training process.

To address the issue of edge devices being unable to train models due to resource limitations,
mothod for training global model's sub-models on edge devices have been proposed.
This paper presents two different methods based on the sub-model extraction approach,
each solving the resource-constrained federated learning problem in scenarios with varying client distribution quality.
we propose Distribution-aware Sub-model Extraction for FL (FedDSE).
Unlike the competition that arises from neurons being optimized in different directions when sub-models are extracted based on predefined strategies,
FedDSE selects neurons with higher activation values on the global model based on the data distribution of client devices.
It forms sub-models by choosing neurons that are activated differently across devices.
This method automatically extracts specific sub-models in each training round based on the varying data distributions of edge devices, 
effectively solving the neuron conflicts caused by sub-model training. 
Extensive experiments demonstrate that FedDSE achieves significant improvements over other methods in scenarios with low-quality client distributions.
Secondly, in high-quality client distribution scenarios, we propose a gradient-based sub-model extraction method (FedGSE), 
which constrains the gradient update directions of the global model and sub-models. 
In each round of updates, this method selects neurons with large gradients during backpropagation from the global model to form sub-models.
This ensures that the updates of the sub-models align closely with the global model updates, 
thus optimizing both the sub-model and the global model in the same direction.
Through comparisons with recent methods on multiple datasets, FedGSE shows significant improvements over traditional approaches.


\textbf{Keywords:} Federated Learning; Resource constrained; 
Sub-model Extraction; Distribution-aware; Gradient Update Optimization